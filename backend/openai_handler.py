"""
OpenAI GPT Handler â€” Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Yandex GPT Ğ½Ğ° OpenAI (GPT-5 Mini)

ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¾Ñ‚ YandexGPTHandler:
- ĞĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Function Calling (tool_calls) â€” ĞĞ• Ğ½ÑƒĞ¶ĞµĞ½ plaintext regex parsing
- Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: role="tool" Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ (Ğ° Ğ½Ğµ role="user")
- OpenAI SDK Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ HTTP Ğº Yandex Completion API

ĞĞ°ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ’Ğ¡Ğ® Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¸Ğ· YandexGPTHandler:
- _dispatch_function (~1200 ÑÑ‚Ñ€Ğ¾Ğº Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ TourVisor API)
- _execute_function (Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ + Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
- _check_cascade_slots (Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ ĞºĞ°ÑĞºĞ°Ğ´Ğ°)
- Ğ’ÑĞµ safety-net Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ (F1-F8, P1-P15, R6-R9, C2, H1-H2)
- _resolve_tourid_from_text, _dialogue_log, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
"""

import os
import re
import json
import asyncio
import time
import logging
from typing import Optional, Dict, List
from openai import OpenAI
from dotenv import load_dotenv

try:
    from yandex_handler import (
        YandexGPTHandler,
        _is_promised_search,
        _dedup_response,
        _strip_reasoning_leak,
        _dedup_sentences,
        _strip_trailing_fragment,
        StreamCallback,
    )
except ImportError:
    from backend.yandex_handler import (
        YandexGPTHandler,
        _is_promised_search,
        _dedup_response,
        _strip_reasoning_leak,
        _dedup_sentences,
        _strip_trailing_fragment,
        StreamCallback,
    )

load_dotenv()

logger = logging.getLogger("mgp_bot")

_RE_FUNC_NAMES = re.compile(
    r'\(?(get_tour_details|search_tours|get_search_results|'
    r'get_search_status|get_hotel_info|actualize_tour|'
    r'get_hot_tours|continue_search|get_dictionaries|'
    r'get_current_date)\)?',
    re.IGNORECASE
)


class OpenAIHandler(YandexGPTHandler):
    """
    OpenAI GPT Handler Ñ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Function Calling.

    ĞĞ°ÑĞ»ĞµĞ´ÑƒĞµÑ‚:
    - _dispatch_function (Ğ²ÑÑ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° TourVisor API)
    - _execute_function (Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ + Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
    - Ğ’ÑĞµ safety-net Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ (F1-F8, P1-P15, R6-R9, C2, H1-H2)
    - _resolve_tourid_from_text, _dialogue_log, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, tour_cards

    ĞŸĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚:
    - __init__ (OpenAI SDK Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Yandex HTTP)
    - chat() (Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ tool_calls Ğ²Ğ¼ĞµÑÑ‚Ğ¾ plaintext parsing)
    - chat_stream() (Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² chat())
    - close_sync(), reset()
    """

    def __init__(self):
        # Initialize all shared state from parent (tourvisor, history, metrics, etc.)
        super().__init__()

        # Validate OpenAI API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ² .env! "
                "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ OPENAI_API_KEY=sk-... Ğ² backend/.env"
            )

        # Override with OpenAI client
        # OPENAI_BASE_URL â€” Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ (ĞµÑĞ»Ğ¸ OpenAI API Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ½Ğ°Ğ¿Ñ€. Ğ¸Ğ· Ğ Ğ¾ÑÑĞ¸Ğ¸)
        base_url = os.getenv("OPENAI_BASE_URL")
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info("ğŸŒ OpenAI proxy: %s", base_url)

        self.openai_client = OpenAI(timeout=120.0, **client_kwargs)
        self.model = os.getenv("OPENAI_MODEL", "gpt-5-mini")

        # Build OpenAI-formatted tools from function_schemas.json
        self.openai_tools = self._build_openai_tools()

        logger.info(
            "ğŸ¤– OpenAIHandler INIT  model=%s  tools=%d",
            self.model, len(self.openai_tools)
        )

    # â”€â”€â”€ Tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_openai_tools(self) -> List[Dict]:
        """
        Convert function_schemas.json to OpenAI tools format.

        Yandex format:  {"type": "function", "name": "...", "parameters": {...}}
        OpenAI format:  {"type": "function", "function": {"name": "...", "parameters": {...}}}
        """
        schema_path = os.path.join(
            os.path.dirname(__file__), "..", "function_schemas.json"
        )
        with open(schema_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        openai_tools = []
        for tool in data.get("tools", []):
            if tool.get("type") == "function":
                openai_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool.get("description", ""),
                        "parameters": tool.get("parameters", {}),
                    }
                })

        logger.info("ğŸ”§ Loaded %d OpenAI tools from function_schemas.json", len(openai_tools))
        return openai_tools

    # â”€â”€â”€ Messages Builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_openai_messages(self) -> List[Dict]:
        """
        Build messages array for OpenAI Chat Completions API.

        Format:
        - {"role": "system", "content": "..."}          â€” system prompt
        - {"role": "user", "content": "..."}             â€” user messages
        - {"role": "assistant", "content": "..."}        â€” text responses
        - {"role": "assistant", "tool_calls": [...]}     â€” function calls
        - {"role": "tool", "tool_call_id": "...", ...}   â€” function results
        """
        messages = []

        # System prompt
        if self.instructions:
            messages.append({"role": "system", "content": self.instructions})

        # Full history
        for item in self.full_history:
            role = item.get("role")

            if role == "user":
                messages.append({
                    "role": "user",
                    "content": item.get("content", "")
                })
            elif role == "assistant":
                msg = {"role": "assistant", "content": item.get("content")}
                if "tool_calls" in item:
                    msg["tool_calls"] = item["tool_calls"]
                messages.append(msg)
            elif role == "tool":
                messages.append({
                    "role": "tool",
                    "tool_call_id": item.get("tool_call_id", ""),
                    "content": item.get("content", "")
                })

        return messages

    # â”€â”€â”€ History Trimming (tool_call-aware) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _trim_history(self):
        """
        Override: trim history while preserving tool_call/tool_result pairs.

        OpenAI API requires that every assistant message with tool_calls
        is immediately followed by tool results for ALL those calls.
        Splitting them causes HTTP 400 errors.
        """
        if len(self.full_history) <= self._max_history_len:
            return

        old_len = len(self.full_history)
        keep_start = 2
        keep_end = self._max_history_len - keep_start
        tail = self.full_history[-keep_end:]

        # Remove orphaned tool messages at the start of tail
        while tail and tail[0].get("role") == "tool":
            tail.pop(0)

        # If tail starts with assistant + tool_calls without complete results, remove it
        if (tail
                and tail[0].get("role") == "assistant"
                and tail[0].get("tool_calls")):
            tc_ids = {tc["id"] for tc in tail[0].get("tool_calls", [])}
            found_ids = set()
            j = 1
            while j < len(tail) and tail[j].get("role") == "tool":
                found_ids.add(tail[j].get("tool_call_id"))
                j += 1
            if tc_ids != found_ids:
                tail = tail[j:]

        self.full_history = self.full_history[:keep_start] + tail
        logger.info(
            "âœ‚ï¸ TRIM full_history: %d â†’ %d messages",
            old_len, len(self.full_history)
        )

    # â”€â”€â”€ OpenAI API Call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _call_openai_sync(self, messages: List[Dict]):
        """
        Synchronous OpenAI API call.
        Run in thread via asyncio.to_thread() to avoid blocking the event loop.
        """
        return self.openai_client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools=self.openai_tools,
            temperature=0.2,
            max_tokens=4096,
        )

    # â”€â”€â”€ Main Chat Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def chat(self, user_message: str) -> str:
        """
        Send message and get response using OpenAI GPT with native tool calling.

        Key differences from YandexGPTHandler.chat():
        - No plaintext function call parsing (tool_calls are native JSON)
        - No content filter bypass (OpenAI doesn't have Yandex's content filter)
        - No role alternation hacks (_append_history not needed)
        - Tool results stored as role="tool" (not role="user")
        """
        # Reset tour cards for this message
        self._pending_tour_cards = []
        self._metrics["total_messages"] += 1

        # Add user message to history
        self.full_history.append({"role": "user", "content": user_message})
        self._trim_history()

        logger.info(
            "ğŸ‘¤ USER >> \"%s\"  full_history=%d  model=%s",
            user_message[:150], len(self.full_history), self.model
        )

        max_iterations = 20
        iteration = 0
        chat_start = time.perf_counter()
        empty_retries = 0
        timeout_retries = 0
        geo_retries = 0

        while iteration < max_iterations:
            iteration += 1
            messages = self._build_openai_messages()

            logger.info(
                "ğŸ”„ ITERATION %d/%d  messages=%d  model=%s",
                iteration, max_iterations, len(messages), self.model
            )

            t0 = time.perf_counter()
            try:
                response = await asyncio.to_thread(
                    self._call_openai_sync, messages
                )
                api_ms = int((time.perf_counter() - t0) * 1000)

                choice = response.choices[0]
                message = choice.message
                finish_reason = choice.finish_reason

                # Token usage logging
                usage = response.usage
                if usage:
                    logger.info(
                        "ğŸ¤– OPENAI API <<  %dms  finish=%s  "
                        "tokens: prompt=%d completion=%d total=%d",
                        api_ms, finish_reason,
                        usage.prompt_tokens, usage.completion_tokens,
                        usage.total_tokens
                    )
                else:
                    logger.info(
                        "ğŸ¤– OPENAI API <<  %dms  finish=%s",
                        api_ms, finish_reason
                    )

            except Exception as e:
                api_ms = int((time.perf_counter() - t0) * 1000)
                error_str = str(e)
                logger.error(
                    "ğŸ¤– OPENAI API !! ERROR  %dms  %s",
                    api_ms, error_str[:300]
                )

                # Rate limit
                if "429" in error_str or "rate_limit" in error_str.lower():
                    return (
                        "Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¶ĞµĞ½. "
                        "ĞŸĞ¾Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ñ‚Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞºÑƒĞ½Ğ´ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚Ğµ."
                    )

                # Token limit exceeded
                if ("context_length_exceeded" in error_str
                        or "maximum context length" in error_str
                        or "max_tokens" in error_str.lower()):
                    logger.warning(
                        "âš ï¸ TOKEN LIMIT EXCEEDED â€” trimming history "
                        "from %d messages",
                        len(self.full_history)
                    )
                    if len(self.full_history) > 8:
                        head = self.full_history[:2]
                        tail = self.full_history[-4:]
                        # Remove orphaned tool messages at start of tail
                        while tail and tail[0].get("role") == "tool":
                            tail.pop(0)
                        self.full_history = head + tail
                        logger.info(
                            "âœ… History trimmed to %d messages",
                            len(self.full_history)
                        )
                    empty_retries += 1
                    if empty_retries < 3:
                        continue
                    return (
                        "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ ÑÑ‚Ğ°Ğ» ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼. "
                        "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ½Ğ°Ñ‡Ğ½Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚."
                    )

                # Invalid request (orphaned tool message, malformed history)
                if "400" in error_str or "invalid" in error_str.lower():
                    logger.warning(
                        "âš ï¸ 400 ERROR â€” attempting history cleanup"
                    )
                    self._cleanup_history()
                    empty_retries += 1
                    if empty_retries < 3:
                        continue

                # Timeout (server-side, e.g. OpenRouter)
                if "timed out" in error_str.lower() or "timeout" in error_str.lower():
                    timeout_retries += 1
                    if timeout_retries < 2:
                        logger.warning(
                            "â±ï¸ TIMEOUT RETRY %d/2 â€” Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· 2Ñ",
                            timeout_retries
                        )
                        await asyncio.sleep(2)
                        continue

                # Geo-blocking (OpenRouter â†’ OpenAI from Russia)
                if ("403" in error_str
                        or "unsupported_country" in error_str
                        or "Forbidden" in error_str):
                    geo_retries += 1
                    if geo_retries < 2:
                        logger.warning(
                            "âš ï¸ 403 GEO-BLOCK RETRY %d/2 â€” Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· 3Ñ",
                            geo_retries
                        )
                        await asyncio.sleep(3)
                        continue

                return (
                    "ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. "
                    "ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ· Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ñ‡Ğ½Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚."
                )

            # â”€â”€ Handle tool calls (native) â”€â”€
            if message.tool_calls:
                # Store assistant message with tool_calls in history
                assistant_msg = {
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            }
                        }
                        for tc in message.tool_calls
                    ]
                }
                self.full_history.append(assistant_msg)

                # Log
                func_names = [tc.function.name for tc in message.tool_calls]
                logger.info(
                    "ğŸ”§ TOOL CALLS: %s", ", ".join(func_names)
                )

                # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ tool calls
                _LARGE_FUNCS = {
                    'get_search_results', 'get_hotel_info', 'get_hot_tours'
                }

                def _truncate_tool_output(func_name, output):
                    limit = 2000 if func_name in _LARGE_FUNCS else 1000
                    if len(output) > limit:
                        return output[:limit] + "â€¦"
                    return output

                if len(message.tool_calls) == 1:
                    tc = message.tool_calls[0]
                    arguments = tc.function.arguments or "{}"
                    result = await self._execute_function(
                        tc.function.name, arguments, tc.id
                    )
                    self.full_history.append({
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": _truncate_tool_output(
                            tc.function.name, result["output"]
                        )
                    })
                else:
                    async def _exec_tool_call(tool_call):
                        args = tool_call.function.arguments or "{}"
                        return (
                            tool_call.id,
                            tool_call.function.name,
                            await self._execute_function(
                                tool_call.function.name, args, tool_call.id
                            )
                        )

                    results = await asyncio.gather(*[
                        _exec_tool_call(tc) for tc in message.tool_calls
                    ])

                    for tc_id, tc_name, result in results:
                        self.full_history.append({
                            "role": "tool",
                            "tool_call_id": tc_id,
                            "content": _truncate_tool_output(
                                tc_name, result["output"]
                            )
                        })

                logger.info(
                    "ğŸ”„ TOOL CALLS DONE  count=%d  continuingâ€¦",
                    len(message.tool_calls)
                )
                continue

            # â”€â”€ Handle text response â”€â”€
            final_text = message.content or ""

            # Content filter (OpenAI)
            if finish_reason == "content_filter":
                empty_retries += 1
                logger.warning(
                    "âš ï¸ CONTENT_FILTER detected (#%d): \"%s\"",
                    empty_retries, final_text[:100]
                )
                if empty_retries >= 3:
                    return (
                        "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. "
                        "ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ."
                    )
                self.full_history.append({
                    "role": "user",
                    "content": (
                        "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ÑŒ "
                        "Ñ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚ÑƒÑ€Ğ°."
                    )
                })
                continue

            # Truncated response (max_tokens) â€” trim to last complete sentence
            if finish_reason == "length" and final_text:
                logger.warning(
                    "âš ï¸ Response truncated (max_tokens). "
                    "Length: %d chars", len(final_text)
                )
                for sep in ['. ', '! ', '? ', '.\n']:
                    idx = final_text.rfind(sep)
                    if idx > len(final_text) * 0.5:
                        final_text = final_text[:idx + 1]
                        break

            # Empty response
            if not final_text:
                empty_retries += 1
                logger.warning(
                    "âš ï¸ EMPTY RESPONSE #%d", empty_retries
                )
                if empty_retries >= 3:
                    if self._pending_tour_cards:
                        return (
                            "Ğ’Ğ¾Ñ‚ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑˆÑ‘Ğ» Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ! "
                            "ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¸ ÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ, "
                            "ĞºĞ°ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ²Ğ°Ğ» â€” Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ñƒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ."
                        )
                    return (
                        "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. "
                        "ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ."
                    )
                self.full_history.append({
                    "role": "user",
                    "content": (
                        "ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾ĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° "
                        "Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                    )
                })
                continue

            # Promised search detection (safety-net)
            if _is_promised_search(final_text):
                empty_retries += 1
                self._metrics["promised_search_detections"] = \
                    self._metrics.get("promised_search_detections", 0) + 1
                logger.warning(
                    "âš ï¸ PROMISED-SEARCH detected (#%d): \"%s\"",
                    empty_retries, final_text[:150]
                )
                if empty_retries < 2:
                    self.full_history.append({
                        "role": "assistant", "content": final_text
                    })
                    self.full_history.append({
                        "role": "user",
                        "content": (
                            "Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞĞĞ¯ ĞĞ¨Ğ˜Ğ‘ĞšĞ: Ğ¢Ñ‹ ĞĞŸĞ˜Ğ¡ĞĞ› Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ "
                            "Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ½Ğ¾ ĞĞ• Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ» Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ. "
                            "ĞĞ•ĞœĞ•Ğ”Ğ›Ğ•ĞĞĞ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸ get_current_date(), "
                            "Ğ·Ğ°Ñ‚ĞµĞ¼ search_tours() Ñ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ "
                            "Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. ĞĞ˜ĞšĞĞ“Ğ”Ğ Ğ½Ğµ Ğ¿Ğ¸ÑˆĞ¸ "
                            "'ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ¿Ğ¾Ğ¸Ñ‰Ñƒ' â€” Ğ’Ğ«Ğ—Ğ«Ğ’ĞĞ™ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ!"
                        )
                    })
                    continue

            # Search pipeline break detection (safety-net)
            if getattr(self, '_search_awaiting_results', False):
                logger.warning(
                    "âš ï¸ SEARCH-PIPELINE-BREAK: model stopped without get_search_results"
                )
                empty_retries += 1
                if empty_retries < 3:
                    self.full_history.append({
                        "role": "assistant", "content": final_text
                    })
                    self.full_history.append({
                        "role": "user",
                        "content": (
                            f"Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞĞĞ¯ ĞĞ¨Ğ˜Ğ‘ĞšĞ: search_tours Ğ²ĞµÑ€Ğ½ÑƒĞ» requestid, "
                            f"Ğ½Ğ¾ Ñ‚Ñ‹ ĞĞ• Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ» get_search_status Ğ¸ get_search_results. "
                            f"ĞĞ•ĞœĞ•Ğ”Ğ›Ğ•ĞĞĞ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸ get_search_status(requestid="
                            f"{self._last_requestid}). ĞĞ• Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ Ğ¿Ğ¾ĞºĞ° "
                            f"Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸ÑˆÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· get_search_results!"
                        )
                    })
                    continue
                else:
                    self._search_awaiting_results = False

            # Result leak detection (safety-net)
            if final_text.lstrip().startswith("Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²"):
                logger.warning("âš ï¸ RESULT-LEAK detected")
                self._metrics.setdefault("result_leak_filtered", 0)
                self._metrics["result_leak_filtered"] += 1
                if self._pending_tour_cards:
                    final_text = (
                        "Ğ’Ğ¾Ñ‚ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑˆÑ‘Ğ» Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ! "
                        "ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¸ ÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ, "
                        "ĞºĞ°ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ²Ğ°Ğ» â€” Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ñƒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ."
                    )
                else:
                    empty_retries += 1
                    if empty_retries < 3:
                        self.full_history.append({
                            "role": "assistant", "content": final_text
                        })
                        self.full_history.append({
                            "role": "user",
                            "content": (
                                "ĞÑ‚Ğ²ĞµÑ‚ÑŒ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ â€” "
                                "ĞĞ• Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ ÑÑ‹Ñ€Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. "
                                "Ğ•ÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ ĞµÑ‰Ñ‘ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ â€” Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸."
                            )
                        })
                        continue
                    final_text = "Ğ¯ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» Ğ²Ğ°Ñˆ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. Ğ§ĞµĞ¼ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ?"

            # Dedup (safety-net, unlikely with OpenAI but harmless)
            final_text = _dedup_response(final_text)

            # Strip leaked LLM reasoning / JSON fragments from end of response
            final_text = _strip_reasoning_leak(final_text)

            # Sentence-level dedup (catches intra-paragraph question repeats)
            final_text = _dedup_sentences(final_text)

            # Strip orphaned dialogue-continuation fragments after last '?'
            final_text = _strip_trailing_fragment(final_text)

            # Strip leaked function names (e.g. "get_tour_details")
            final_text = _RE_FUNC_NAMES.sub('', final_text)
            final_text = re.sub(r'\s{2,}', ' ', final_text).strip()

            # Save to history
            self.full_history.append({
                "role": "assistant", "content": final_text
            })

            total_ms = int((time.perf_counter() - chat_start) * 1000)
            logger.info(
                "ğŸ¤– ASSISTANT << %d chars  %d iterations  %dms total  \"%s\"",
                len(final_text), iteration, total_ms,
                final_text[:200] + ("â€¦" if len(final_text) > 200 else "")
            )
            return final_text

        logger.error("ğŸ¤– MAX ITERATIONS REACHED (%d)", max_iterations)
        return (
            "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. "
            "ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ· Ğ¸Ğ»Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹."
        )

    # â”€â”€â”€ History Cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _cleanup_history(self):
        """
        Remove invalid message sequences from full_history.
        Fixes orphaned tool messages and incomplete tool_call groups.
        """
        cleaned = []
        i = 0
        while i < len(self.full_history):
            msg = self.full_history[i]

            if msg.get("role") == "tool":
                # Only keep if previous is assistant with matching tool_calls
                if (cleaned
                        and cleaned[-1].get("role") == "assistant"
                        and cleaned[-1].get("tool_calls")):
                    tc_ids = {
                        tc["id"]
                        for tc in cleaned[-1]["tool_calls"]
                    }
                    if msg.get("tool_call_id") in tc_ids:
                        cleaned.append(msg)
                        i += 1
                        continue
                # Orphaned tool message â€” skip
                logger.debug(
                    "ğŸ§¹ CLEANUP: skipping orphaned tool message "
                    "tool_call_id=%s",
                    msg.get("tool_call_id", "?")
                )
                i += 1
                continue

            cleaned.append(msg)
            i += 1

        if len(cleaned) != len(self.full_history):
            logger.info(
                "ğŸ§¹ CLEANUP: %d â†’ %d messages (removed %d invalid)",
                len(self.full_history), len(cleaned),
                len(self.full_history) - len(cleaned)
            )
        self.full_history = cleaned

    # â”€â”€â”€ Streaming (fallback to non-streaming) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def chat_stream(
        self,
        user_message: str,
        on_token: Optional[StreamCallback] = None
    ) -> str:
        """
        Streaming not yet implemented for OpenAI.
        Falls back to regular chat().
        """
        logger.warning(
            "âš ï¸ chat_stream() fallback to chat() â€” "
            "streaming Ğ½Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ OpenAI"
        )
        result = await self.chat(user_message)
        if on_token:
            on_token(result)
        return result

    # â”€â”€â”€ Lifecycle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def close_sync(self):
        """Close OpenAI client resources."""
        try:
            self.openai_client.close()
        except Exception:
            pass

    def reset(self):
        """Reset dialogue history and all caches."""
        old_len = len(self.full_history)
        self.full_history = []
        self.input_list = []
        self._pending_tour_cards = []
        self._last_departure_city = "ĞœĞ¾ÑĞºĞ²Ğ°"
        self._last_requestid = None
        self._tourid_map = {}
        self._last_search_params = {}
        self._user_stated_budget = None
        self._empty_iterations = 0
        self.previous_response_id = None
        self._metrics = {
            "promised_search_detections": 0,
            "cascade_incomplete_detections": 0,
            "dateto_corrections": 0,
            "total_searches": 0,
            "total_messages": 0,
        }
        logger.info(
            "ğŸ”„ HANDLER RESET  cleared %d messages from full_history",
            old_len
        )
