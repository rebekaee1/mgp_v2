"""
OpenAI GPT Handler ‚Äî –º–∏–≥—Ä–∞—Ü–∏—è —Å Yandex GPT –Ω–∞ OpenAI (GPT-5 Mini)

–ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç YandexGPTHandler:
- –ù–∞—Ç–∏–≤–Ω—ã–π Function Calling (tool_calls) ‚Äî –ù–ï –Ω—É–∂–µ–Ω plaintext regex parsing
- –§–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π: role="tool" –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π (–∞ –Ω–µ role="user")
- OpenAI SDK –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ HTTP –∫ Yandex Completion API

–ù–∞—Å–ª–µ–¥—É–µ—Ç –í–°–Æ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É –∏–∑ YandexGPTHandler:
- _dispatch_function (~1200 —Å—Ç—Ä–æ–∫ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ TourVisor API)
- _execute_function (–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ + –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)
- _check_cascade_slots (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –∫–∞—Å–∫–∞–¥–∞)
- –í—Å–µ safety-net –ø—Ä–∞–≤–∫–∏ (F1-F8, P1-P15, R6-R9, C2, H1-H2)
- _resolve_tourid_from_text, _dialogue_log, –º–µ—Ç—Ä–∏–∫–∏
"""

import os
import re
import json
import asyncio
import time
import logging
from typing import Optional, Dict, List
from openai import OpenAI
from dotenv import load_dotenv

try:
    from yandex_handler import (
        YandexGPTHandler,
        _is_promised_search,
        _dedup_response,
        _strip_reasoning_leak,
        _dedup_sentences,
        _strip_trailing_fragment,
        StreamCallback,
    )
except ImportError:
    from backend.yandex_handler import (
        YandexGPTHandler,
        _is_promised_search,
        _dedup_response,
        _strip_reasoning_leak,
        _dedup_sentences,
        _strip_trailing_fragment,
        StreamCallback,
    )

load_dotenv()

logger = logging.getLogger("mgp_bot")

_RE_FUNC_NAMES = re.compile(
    r'\(?(get_tour_details|search_tours|get_search_results|'
    r'get_search_status|get_hotel_info|actualize_tour|'
    r'get_hot_tours|continue_search|get_dictionaries|'
    r'get_current_date)\)?',
    re.IGNORECASE
)


class OpenAIHandler(YandexGPTHandler):
    """
    OpenAI GPT Handler —Å –Ω–∞—Ç–∏–≤–Ω—ã–º Function Calling.

    –ù–∞—Å–ª–µ–¥—É–µ—Ç:
    - _dispatch_function (–≤—Å—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ TourVisor API)
    - _execute_function (–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ + –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)
    - –í—Å–µ safety-net –ø—Ä–∞–≤–∫–∏ (F1-F8, P1-P15, R6-R9, C2, H1-H2)
    - _resolve_tourid_from_text, _dialogue_log, –º–µ—Ç—Ä–∏–∫–∏, tour_cards

    –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
    - __init__ (OpenAI SDK –≤–º–µ—Å—Ç–æ Yandex HTTP)
    - chat() (–Ω–∞—Ç–∏–≤–Ω—ã–µ tool_calls –≤–º–µ—Å—Ç–æ plaintext parsing)
    - chat_stream() (–¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç –≤ chat())
    - close_sync(), reset()
    """

    def __init__(self):
        # Initialize all shared state from parent (tourvisor, history, metrics, etc.)
        super().__init__()

        # Validate OpenAI API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY –Ω–µ —É–∫–∞–∑–∞–Ω –≤ .env! "
                "–î–æ–±–∞–≤—å—Ç–µ OPENAI_API_KEY=sk-... –≤ backend/.env"
            )

        # Override with OpenAI client
        # OPENAI_BASE_URL ‚Äî –¥–ª—è –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ OpenAI API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞–ø—Ä—è–º—É—é, –Ω–∞–ø—Ä. –∏–∑ –†–æ—Å—Å–∏–∏)
        base_url = os.getenv("OPENAI_BASE_URL")
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info("üåê OpenAI proxy: %s", base_url)

        self.openai_client = OpenAI(timeout=120.0, **client_kwargs)
        self.model = os.getenv("OPENAI_MODEL", "gpt-5-mini")

        # Pinned context survives history trimming (tour cards summary)
        self._pinned_context: Optional[str] = None
        # Pinned search intent survives trimming (e.g. "–±–µ–∑ –ø–µ—Ä–µ–ª—ë—Ç–∞")
        self._pinned_search_intent: Optional[str] = None
        # Collected cascade slots ‚Äî injected as system message to prevent "forgetting"
        self._collected_slots: Dict[str, str] = {}

        # Build OpenAI-formatted tools from function_schemas.json
        self.openai_tools = self._build_openai_tools()

        logger.info(
            "ü§ñ OpenAIHandler INIT  model=%s  tools=%d",
            self.model, len(self.openai_tools)
        )

    # ‚îÄ‚îÄ‚îÄ Tools ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_openai_tools(self) -> List[Dict]:
        """
        Convert function_schemas.json to OpenAI tools format.

        Yandex format:  {"type": "function", "name": "...", "parameters": {...}}
        OpenAI format:  {"type": "function", "function": {"name": "...", "parameters": {...}}}
        """
        schema_path = os.path.join(
            os.path.dirname(__file__), "..", "function_schemas.json"
        )
        with open(schema_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        openai_tools = []
        for tool in data.get("tools", []):
            if tool.get("type") == "function":
                openai_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool.get("description", ""),
                        "parameters": tool.get("parameters", {}),
                    }
                })

        logger.info("üîß Loaded %d OpenAI tools from function_schemas.json", len(openai_tools))
        return openai_tools

    # ‚îÄ‚îÄ‚îÄ Messages Builder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_openai_messages(self) -> List[Dict]:
        """
        Build messages array for OpenAI Chat Completions API.

        Format:
        - {"role": "system", "content": "..."}          ‚Äî system prompt
        - {"role": "user", "content": "..."}             ‚Äî user messages
        - {"role": "assistant", "content": "..."}        ‚Äî text responses
        - {"role": "assistant", "tool_calls": [...]}     ‚Äî function calls
        - {"role": "tool", "tool_call_id": "...", ...}   ‚Äî function results
        """
        messages = []

        # System prompt
        if self.instructions:
            messages.append({"role": "system", "content": self.instructions})

        # Pinned context (tour cards summary ‚Äî survives trimming)
        if self._pinned_context:
            messages.append({
                "role": "system",
                "content": self._pinned_context
            })

        # Pinned search intent (e.g. "–±–µ–∑ –ø–µ—Ä–µ–ª—ë—Ç–∞" ‚Äî survives trimming)
        if self._pinned_search_intent:
            messages.append({
                "role": "system",
                "content": self._pinned_search_intent
            })

        # Collected cascade slots reminder (prevents model from re-asking known params)
        if self._collected_slots:
            slot_lines = [f"- {k}: {v}" for k, v in self._collected_slots.items()]
            messages.append({
                "role": "system",
                "content": (
                    "[–°–û–ë–†–ê–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ö–õ–ò–ï–ù–¢–ê ‚Äî –ù–ï –ø–µ—Ä–µ—Å–ø—Ä–∞—à–∏–≤–∞–π]\n"
                    + "\n".join(slot_lines)
                    + "\n–ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –ù–ï –º–µ–Ω—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ."
                )
            })

        # Full history
        for item in self.full_history:
            role = item.get("role")

            if role == "user":
                messages.append({
                    "role": "user",
                    "content": item.get("content", "")
                })
            elif role == "assistant":
                msg = {"role": "assistant", "content": item.get("content")}
                if "tool_calls" in item:
                    msg["tool_calls"] = item["tool_calls"]
                messages.append(msg)
            elif role == "tool":
                messages.append({
                    "role": "tool",
                    "tool_call_id": item.get("tool_call_id", ""),
                    "content": item.get("content", "")
                })

        return messages

    # ‚îÄ‚îÄ‚îÄ Slot Tracker ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    _SLOT_PATTERNS = {
        "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": [
            (r'\b(?:—Ç—É—Ä—Ü–∏[—è—é–∏]|–µ–≥–∏–ø–µ?—Ç|–æ–∞—ç|—ç–º–∏—Ä–∞—Ç|—Ç–∞–∏–ª–∞–Ω–¥|–º–∞–ª—å–¥–∏–≤|–≥—Ä–µ—Ü–∏|–∫–∏–ø—Ä|'
             r'–≤—å–µ—Ç–Ω–∞–º|—à—Ä–∏.?–ª–∞–Ω–∫|–∫—É–±[–∞–µ—É]|–¥–æ–º–∏–Ω–∏–∫–∞–Ω|–∏–Ω–¥–æ–Ω–µ–∑–∏|–±–∞–ª–∏|—Ç—É–Ω–∏—Å|'
             r'—á–µ—Ä–Ω–æ–≥–æ—Ä–∏|–±–æ–ª–≥–∞—Ä–∏|—Ö–æ—Ä–≤–∞—Ç–∏|–∞–±—Ö–∞–∑–∏|—Ä–æ—Å—Å–∏|—Å–æ—á–∏|–∫—Ä—ã–º|–∞–Ω–∞–ø|'
             r'–≥–µ–ª–µ–Ω–¥–∂–∏–∫|–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥|–∫–º–≤|–º–∞—Ä–æ–∫–∫|–∏–∑—Ä–∞–∏–ª|–∏–æ—Ä–¥–∞–Ω–∏|'
             r'–∏–Ω–¥–∏—è|–∫–∏—Ç–∞–π|—è–ø–æ–Ω–∏|—é–∂–Ω–∞—è –∫–æ—Ä–µ—è|–º–µ–∫—Å–∏–∫|–±—Ä–∞–∑–∏–ª–∏)\w*', None),
        ],
        "–ì–æ—Ä–æ–¥ –≤—ã–ª–µ—Ç–∞": [
            (r'\b(?:–º–æ—Å–∫–≤|–ø–∏—Ç–µ—Ä|—Å–ø–±|—Å–∞–Ω–∫—Ç.?–ø–µ—Ç–µ—Ä–±—É—Ä–≥|–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥|–µ–∫–±|–∫–∞–∑–∞–Ω[—å–∏]|'
             r'–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫|–Ω—Å–∫|–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä|–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫|—Ä–æ—Å—Ç–æ–≤|—É—Ñ[–∞–µ—ã]|–ø–µ—Ä–º—å?|'
             r'—á–µ–ª—è–±–∏–Ω—Å–∫|—Å–∞–º–∞—Ä[–∞–µ—É]|–Ω–∏–∂–Ω\w+ –Ω–æ–≤–≥–æ—Ä–æ–¥|—Å–æ—á–∏)\w*', None),
            (r'–±–µ–∑\s*–ø–µ—Ä–µ–ª[–µ—ë]—Ç', "–±–µ–∑ –ø–µ—Ä–µ–ª—ë—Ç–∞"),
        ],
        "–î–∞—Ç—ã": [
            (r'(\d{1,2}[./]\d{1,2}(?:[./]\d{2,4})?)', None),
            (r'(\d{1,2})\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|'
             r'—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)', None),
        ],
        "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å": [
            (r'(\d+)\s*(?:–Ω–æ—á|–Ω–æ—á–µ–π)', None),
            (r'(\d+)\s*(?:–¥–Ω|–¥–Ω–µ–π|–¥–µ–Ω—å)', None),
            (r'(?:–Ω–∞\s+)?(?:–Ω–µ–¥–µ–ª—é|–Ω–µ–¥–µ–ª—å–∫—É)', "7 –Ω–æ—á–µ–π"),
            (r'(?:–¥–≤–µ\s+–Ω–µ–¥–µ–ª–∏|2\s+–Ω–µ–¥–µ–ª–∏)', "14 –Ω–æ—á–µ–π"),
        ],
        "–°–æ—Å—Ç–∞–≤": [
            (r'(?:(\d+)\s*(?:–≤–∑—Ä–æ—Å–ª—ã|–≤–∑—Ä))', None),
            (r'(?:–≤–¥–≤–æ[–µ—ë]–º|—Å (?:–º—É–∂–µ–º|–∂–µ–Ω–æ–π|–ø–∞—Ä–Ω–µ–º|–¥–µ–≤—É—à–∫–æ–π))', "2 –≤–∑—Ä–æ—Å–ª—ã—Ö"),
        ],
        "–î–µ—Ç–∏": [
            (r'(\d+)\s*(?:—Ä–µ–±—ë–Ω|—Ä–µ–±–µ–Ω|–¥–µ—Ç)', None),
            (r'(?:–±–µ–∑\s*–¥–µ—Ç–µ–π)', "–±–µ–∑ –¥–µ—Ç–µ–π"),
        ],
        "–í–æ–∑—Ä–∞—Å—Ç —Ä–µ–±—ë–Ω–∫–∞": [
            (r'^(\d{1,2})$', None),
        ],
        "–ü–∏—Ç–∞–Ω–∏–µ": [
            (r'(?:–≤—Å[–µ—ë]\s*–≤–∫–ª—é—á–µ–Ω|all\s*inclusive|–æ–ª–ª\s*–∏–Ω–∫–ª—é–∑–∏–≤)', "–≤—Å—ë –≤–∫–ª—é—á–µ–Ω–æ"),
            (r'(?:–∑–∞–≤—Ç—Ä–∞–∫)', "–∑–∞–≤—Ç—Ä–∞–∫–∏"),
            (r'(?:–ø–æ–ª—É–ø–∞–Ω—Å–∏–æ–Ω)', "–ø–æ–ª—É–ø–∞–Ω—Å–∏–æ–Ω"),
            (r'(?:–ø–æ–ª–Ω—ã–π\s*–ø–∞–Ω—Å–∏–æ–Ω)', "–ø–æ–ª–Ω—ã–π –ø–∞–Ω—Å–∏–æ–Ω"),
        ],
        "–ó–≤—ë–∑–¥–Ω–æ—Å—Ç—å": [
            (r'(\d)\s*(?:–∑–≤—ë–∑–¥|–∑–≤–µ–∑–¥|‚òÖ|\*)', None),
            (r'\b(–ª—é–±\w+)\b.*(?:–∑–≤—ë–∑–¥|–∑–≤–µ–∑–¥|‚òÖ|\*|–∫–∞—Ç–µ–≥–æ—Ä–∏|–≤–∞—Ä–∏–∞–Ω—Ç)', "–ª—é–±–∞—è"),
        ],
    }

    def _update_collected_slots(self, user_message: str):
        """Extract and pin cascade parameters from user messages."""
        text = user_message.lower().strip()
        for slot_name, patterns in self._SLOT_PATTERNS.items():
            for pattern, fixed_value in patterns:
                m = re.search(pattern, text, re.IGNORECASE)
                if m:
                    value = fixed_value or m.group(0)
                    self._collected_slots[slot_name] = value
                    break

        # Context-aware: bare "–ª—é–±–æ–π/–ª—é–±–∞—è/–±–µ–∑ —Ä–∞–∑–Ω–∏—Ü—ã" ‚Üí check what model asked
        if re.match(r'^(?:–ª—é–±–æ–π|–ª—é–±–∞—è|–ª—é–±—ã–µ|–±–µ–∑ —Ä–∞–∑–Ω–∏—Ü—ã|–≤—Å–µ —Ä–∞–≤–Ω–æ|–≤—Å—ë —Ä–∞–≤–Ω–æ|–Ω–µ–≤–∞–∂–Ω–æ|–Ω–µ –≤–∞–∂–Ω–æ)$', text):
            last_assistant = ""
            for msg in reversed(self.full_history):
                if msg.get("role") == "assistant" and msg.get("content"):
                    last_assistant = msg["content"].lower()
                    break
            if any(w in last_assistant for w in ("–∑–≤—ë–∑–¥", "–∑–≤–µ–∑–¥", "–∫–∞—Ç–µ–≥–æ—Ä–∏", "‚òÖ")):
                self._collected_slots["–ó–≤—ë–∑–¥–Ω–æ—Å—Ç—å"] = "–ª—é–±–∞—è"
            elif any(w in last_assistant for w in ("–ø–∏—Ç–∞–Ω–∏", "meal")):
                self._collected_slots["–ü–∏—Ç–∞–Ω–∏–µ"] = "–ª—é–±–æ–µ"

        if self._collected_slots:
            logger.debug("üìå SLOTS: %s", self._collected_slots)

    # ‚îÄ‚îÄ‚îÄ History Trimming (tool_call-aware) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @staticmethod
    def _group_into_blocks(messages):
        """Group messages into atomic blocks: tool_call assistant + its tool results stay together."""
        blocks = []
        i = 0
        while i < len(messages):
            msg = messages[i]
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                block = [msg]
                j = i + 1
                while j < len(messages) and messages[j].get("role") == "tool":
                    block.append(messages[j])
                    j += 1
                blocks.append(block)
                i = j
            else:
                blocks.append([msg])
                i += 1
        return blocks

    def _trim_history(self):
        """
        Trim history while preserving tool_call/tool_result pairs as atomic blocks.
        Removes oldest non-system blocks until under the limit.
        """
        if len(self.full_history) <= self._max_history_len:
            return

        old_len = len(self.full_history)
        blocks = self._group_into_blocks(self.full_history)

        total = sum(len(b) for b in blocks)
        while total > self._max_history_len and len(blocks) > 3:
            removed = blocks.pop(1)
            total -= len(removed)

        self.full_history = [msg for block in blocks for msg in block]
        logger.info(
            "‚úÇÔ∏è TRIM full_history: %d ‚Üí %d messages",
            old_len, len(self.full_history)
        )

    # ‚îÄ‚îÄ‚îÄ OpenAI API Call ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _call_openai_sync(self, messages: List[Dict]):
        """
        Synchronous OpenAI API call.
        Run in thread via asyncio.to_thread() to avoid blocking the event loop.
        """
        return self.openai_client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools=self.openai_tools,
            temperature=0.2,
            max_tokens=4096,
            extra_body={"reasoning_effort": "low"},
        )

    # ‚îÄ‚îÄ‚îÄ Main Chat Loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def chat(self, user_message: str) -> str:
        """
        Send message and get response using OpenAI GPT with native tool calling.

        Key differences from YandexGPTHandler.chat():
        - No plaintext function call parsing (tool_calls are native JSON)
        - No content filter bypass (OpenAI doesn't have Yandex's content filter)
        - No role alternation hacks (_append_history not needed)
        - Tool results stored as role="tool" (not role="user")
        """
        # Reset tour cards for this message
        self._pending_tour_cards = []
        self._metrics["total_messages"] += 1

        # Add user message to history
        self.full_history.append({"role": "user", "content": user_message})
        self._trim_history()

        # Track collected cascade slots from user message
        self._update_collected_slots(user_message)

        # Detect and pin "–±–µ–∑ –ø–µ—Ä–µ–ª—ë—Ç–∞" intent so it survives trimming
        if re.search(r'–±–µ–∑\s*–ø–µ—Ä–µ–ª[–µ—ë]—Ç', user_message, re.IGNORECASE):
            self._pinned_search_intent = "[–ü–ê–†–ê–ú–ï–¢–† –ö–õ–ò–ï–ù–¢–ê: —Ç—É—Ä –ë–ï–ó –ü–ï–†–ï–õ–Å–¢–ê (departure=99). –ù–ï —Å–ø—Ä–∞—à–∏–≤–∞–π –≥–æ—Ä–æ–¥ –≤—ã–ª–µ—Ç–∞.]"
            logger.info("üìå Pinned search intent: –±–µ–∑ –ø–µ—Ä–µ–ª—ë—Ç–∞")

        logger.info(
            "üë§ USER >> \"%s\"  full_history=%d  model=%s",
            user_message[:150], len(self.full_history), self.model
        )

        max_iterations = 20
        iteration = 0
        chat_start = time.perf_counter()
        empty_retries = 0
        timeout_retries = 0
        geo_retries = 0

        while iteration < max_iterations:
            iteration += 1
            messages = self._build_openai_messages()

            logger.info(
                "üîÑ ITERATION %d/%d  messages=%d  model=%s",
                iteration, max_iterations, len(messages), self.model
            )

            t0 = time.perf_counter()
            try:
                response = await asyncio.to_thread(
                    self._call_openai_sync, messages
                )
                api_ms = int((time.perf_counter() - t0) * 1000)

                choice = response.choices[0]
                message = choice.message
                finish_reason = choice.finish_reason

                # Token usage logging
                usage = response.usage
                if usage:
                    logger.info(
                        "ü§ñ OPENAI API <<  %dms  finish=%s  "
                        "tokens: prompt=%d completion=%d total=%d",
                        api_ms, finish_reason,
                        usage.prompt_tokens, usage.completion_tokens,
                        usage.total_tokens
                    )
                else:
                    logger.info(
                        "ü§ñ OPENAI API <<  %dms  finish=%s",
                        api_ms, finish_reason
                    )

            except Exception as e:
                api_ms = int((time.perf_counter() - t0) * 1000)
                error_str = str(e)
                logger.error(
                    "ü§ñ OPENAI API !! ERROR  %dms  %s",
                    api_ms, error_str[:300]
                )

                # Rate limit
                if "429" in error_str or "rate_limit" in error_str.lower():
                    return (
                        "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω. "
                        "–ü–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ."
                    )

                # Token limit exceeded
                if ("context_length_exceeded" in error_str
                        or "maximum context length" in error_str
                        or "max_tokens" in error_str.lower()):
                    logger.warning(
                        "‚ö†Ô∏è TOKEN LIMIT EXCEEDED ‚Äî trimming history "
                        "from %d messages",
                        len(self.full_history)
                    )
                    if len(self.full_history) > 8:
                        blocks = self._group_into_blocks(self.full_history)
                        head_blocks = blocks[:1]
                        tail_blocks = blocks[-3:] if len(blocks) > 3 else blocks[1:]
                        self.full_history = [
                            m for b in (head_blocks + tail_blocks) for m in b
                        ]
                        logger.info(
                            "‚úÖ History trimmed to %d messages",
                            len(self.full_history)
                        )
                    empty_retries += 1
                    if empty_retries < 3:
                        continue
                    return (
                        "–ò–∑–≤–∏–Ω–∏—Ç–µ, –¥–∏–∞–ª–æ–≥ —Å—Ç–∞–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º. "
                        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞—á–Ω–∏—Ç–µ –Ω–æ–≤—ã–π —á–∞—Ç."
                    )

                # Invalid request (orphaned tool message, malformed history)
                if "400" in error_str or "invalid" in error_str.lower():
                    logger.warning(
                        "‚ö†Ô∏è 400 ERROR ‚Äî attempting history cleanup"
                    )
                    self._cleanup_history()
                    empty_retries += 1
                    if empty_retries < 3:
                        continue

                # Timeout (server-side, e.g. OpenRouter)
                if "timed out" in error_str.lower() or "timeout" in error_str.lower():
                    timeout_retries += 1
                    if timeout_retries < 2:
                        logger.warning(
                            "‚è±Ô∏è TIMEOUT RETRY %d/2 ‚Äî –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 2—Å",
                            timeout_retries
                        )
                        await asyncio.sleep(2)
                        continue

                # Geo-blocking (OpenRouter ‚Üí OpenAI from Russia)
                if ("403" in error_str
                        or "unsupported_country" in error_str
                        or "Forbidden" in error_str):
                    geo_retries += 1
                    if geo_retries < 2:
                        logger.warning(
                            "‚ö†Ô∏è 403 GEO-BLOCK RETRY %d/2 ‚Äî –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 3—Å",
                            geo_retries
                        )
                        await asyncio.sleep(3)
                        continue

                # Connection reset (OpenRouter drops long requests)
                if any(kw in error_str for kw in (
                    "ConnectionReset", "RemoteDisconnected",
                    "Connection reset", "connection reset",
                    "ConnectionError", "RemoteProtocolError",
                )):
                    timeout_retries += 1
                    if timeout_retries < 2:
                        logger.warning(
                            "üîå CONNECTION RESET RETRY %d/2 ‚Äî –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 3—Å",
                            timeout_retries
                        )
                        await asyncio.sleep(3)
                        continue

                return (
                    "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞. "
                    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –Ω–∞—á–Ω–∏—Ç–µ –Ω–æ–≤—ã–π —á–∞—Ç."
                )

            # ‚îÄ‚îÄ Handle tool calls (native) ‚îÄ‚îÄ
            if message.tool_calls:
                # Store assistant message with tool_calls in history
                assistant_msg = {
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            }
                        }
                        for tc in message.tool_calls
                    ]
                }
                self.full_history.append(assistant_msg)

                # Log
                func_names = [tc.function.name for tc in message.tool_calls]
                logger.info(
                    "üîß TOOL CALLS: %s", ", ".join(func_names)
                )

                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ tool calls
                _LARGE_FUNCS = {
                    'get_search_results', 'get_hotel_info', 'get_hot_tours'
                }

                def _truncate_tool_output(func_name, output):
                    limit = 2000 if func_name in _LARGE_FUNCS else 1000
                    if len(output) > limit:
                        return output[:limit] + "‚Ä¶"
                    return output

                if len(message.tool_calls) == 1:
                    tc = message.tool_calls[0]
                    arguments = tc.function.arguments or "{}"
                    result = await self._execute_function(
                        tc.function.name, arguments, tc.id
                    )
                    self.full_history.append({
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": _truncate_tool_output(
                            tc.function.name, result["output"]
                        )
                    })
                else:
                    async def _exec_tool_call(tool_call):
                        args = tool_call.function.arguments or "{}"
                        return (
                            tool_call.id,
                            tool_call.function.name,
                            await self._execute_function(
                                tool_call.function.name, args, tool_call.id
                            )
                        )

                    results = await asyncio.gather(*[
                        _exec_tool_call(tc) for tc in message.tool_calls
                    ])

                    for tc_id, tc_name, result in results:
                        self.full_history.append({
                            "role": "tool",
                            "tool_call_id": tc_id,
                            "content": _truncate_tool_output(
                                tc_name, result["output"]
                            )
                        })

                logger.info(
                    "üîÑ TOOL CALLS DONE  count=%d  continuing‚Ä¶",
                    len(message.tool_calls)
                )

                # Update pinned context when tour cards are available
                if self._tourid_map:
                    lines = ["[–ö–û–ù–¢–ï–ö–°–¢: —Ç–µ–∫—É—â–∏–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç—É—Ä—ã]"]
                    for pos, entry in sorted(self._tourid_map.items()):
                        lines.append(
                            f"{pos}. {entry.get('hotelname', '?')} "
                            f"(tourid={entry['tourid']}, "
                            f"hotelcode={entry.get('hotelcode', '?')})"
                        )
                    self._pinned_context = "\n".join(lines)

                continue

            # ‚îÄ‚îÄ Handle text response ‚îÄ‚îÄ
            final_text = message.content or ""

            # Content filter (OpenAI)
            if finish_reason == "content_filter":
                empty_retries += 1
                logger.warning(
                    "‚ö†Ô∏è CONTENT_FILTER detected (#%d): \"%s\"",
                    empty_retries, final_text[:100]
                )
                if empty_retries >= 3:
                    return (
                        "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. "
                        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."
                    )
                self.full_history.append({
                    "role": "user",
                    "content": (
                        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–¥–æ–ª–∂–∏ –ø–æ–º–æ–≥–∞—Ç—å "
                        "—Å –ø–æ–¥–±–æ—Ä–æ–º —Ç—É—Ä–∞."
                    )
                })
                continue

            # Truncated response (max_tokens) ‚Äî trim to last complete sentence
            if finish_reason == "length" and final_text:
                logger.warning(
                    "‚ö†Ô∏è Response truncated (max_tokens). "
                    "Length: %d chars", len(final_text)
                )
                for sep in ['. ', '! ', '? ', '.\n']:
                    idx = final_text.rfind(sep)
                    if idx > len(final_text) * 0.5:
                        final_text = final_text[:idx + 1]
                        break

            # Empty response
            if not final_text:
                empty_retries += 1
                logger.warning(
                    "‚ö†Ô∏è EMPTY RESPONSE #%d", empty_retries
                )
                if empty_retries >= 3:
                    if self._pending_tour_cards:
                        return (
                            "–í–æ—Ç —á—Ç–æ –Ω–∞—à—ë–ª –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É! "
                            "–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Å–∫–∞–∂–∏—Ç–µ, "
                            "–∫–∞–∫–æ–π –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–ª ‚Äî —Ä–∞—Å—Å–∫–∞–∂—É –ø–æ–¥—Ä–æ–±–Ω–µ–µ."
                        )
                    return (
                        "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å. "
                        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å."
                    )
                self.full_history.append({
                    "role": "user",
                    "content": (
                        "–ü—Ä–æ–¥–æ–ª–∂–∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –º–æ–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ "
                        "–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
                    )
                })
                continue

            # Promised search detection (safety-net)
            if _is_promised_search(final_text):
                empty_retries += 1
                self._metrics["promised_search_detections"] = \
                    self._metrics.get("promised_search_detections", 0) + 1
                logger.warning(
                    "‚ö†Ô∏è PROMISED-SEARCH detected (#%d): \"%s\"",
                    empty_retries, final_text[:150]
                )
                if empty_retries < 2:
                    self.full_history.append({
                        "role": "assistant", "content": final_text
                    })
                    self.full_history.append({
                        "role": "user",
                        "content": (
                            "–°–ò–°–¢–ï–ú–ù–ê–Ø –û–®–ò–ë–ö–ê: –¢—ã –û–ü–ò–°–ê–õ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ "
                            "–ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–º, –Ω–æ –ù–ï –≤—ã–∑–≤–∞–ª —Ñ—É–Ω–∫—Ü–∏—é. "
                            "–ù–ï–ú–ï–î–õ–ï–ù–ù–û –≤—ã–∑–æ–≤–∏ get_current_date(), "
                            "–∑–∞—Ç–µ–º search_tours() —Å —Å–æ–±—Ä–∞–Ω–Ω—ã–º–∏ "
                            "–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –ù–ò–ö–û–ì–î–ê –Ω–µ –ø–∏—à–∏ "
                            "'—Å–µ–π—á–∞—Å –ø–æ–∏—â—É' ‚Äî –í–´–ó–´–í–ê–ô —Ñ—É–Ω–∫—Ü–∏—é!"
                        )
                    })
                    continue

            # Search pipeline break detection (safety-net)
            if getattr(self, '_search_awaiting_results', False):
                logger.warning(
                    "‚ö†Ô∏è SEARCH-PIPELINE-BREAK: model stopped without get_search_results"
                )
                empty_retries += 1
                if empty_retries < 3:
                    self.full_history.append({
                        "role": "assistant", "content": final_text
                    })
                    self.full_history.append({
                        "role": "user",
                        "content": (
                            f"–°–ò–°–¢–ï–ú–ù–ê–Ø –û–®–ò–ë–ö–ê: search_tours –≤–µ—Ä–Ω—É–ª requestid, "
                            f"–Ω–æ —Ç—ã –ù–ï –≤—ã–∑–≤–∞–ª get_search_status –∏ get_search_results. "
                            f"–ù–ï–ú–ï–î–õ–ï–ù–ù–û –≤—ã–∑–æ–≤–∏ get_search_status(requestid="
                            f"{self._last_requestid}). –ù–ï –æ—Ç–≤–µ—á–∞–π –∫–ª–∏–µ–Ω—Ç—É –ø–æ–∫–∞ "
                            f"–Ω–µ –ø–æ–ª—É—á–∏—à—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ get_search_results!"
                        )
                    })
                    continue
                else:
                    self._search_awaiting_results = False

            # Result leak detection (safety-net)
            if final_text.lstrip().startswith("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤"):
                logger.warning("‚ö†Ô∏è RESULT-LEAK detected")
                self._metrics.setdefault("result_leak_filtered", 0)
                self._metrics["result_leak_filtered"] += 1
                if self._pending_tour_cards:
                    final_text = (
                        "–í–æ—Ç —á—Ç–æ –Ω–∞—à—ë–ª –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É! "
                        "–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ —Å–∫–∞–∂–∏—Ç–µ, "
                        "–∫–∞–∫–æ–π –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–ª ‚Äî —Ä–∞—Å—Å–∫–∞–∂—É –ø–æ–¥—Ä–æ–±–Ω–µ–µ."
                    )
                else:
                    empty_retries += 1
                    if empty_retries < 3:
                        self.full_history.append({
                            "role": "assistant", "content": final_text
                        })
                        self.full_history.append({
                            "role": "user",
                            "content": (
                                "–û—Ç–≤–µ—Ç—å –∫–ª–∏–µ–Ω—Ç—É –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º ‚Äî "
                                "–ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–π —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–π. "
                                "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –µ—â—ë —Ñ—É–Ω–∫—Ü–∏—é ‚Äî –≤—ã–∑–æ–≤–∏."
                            )
                        })
                        continue
                    final_text = "–Ø –æ–±—Ä–∞–±–æ—Ç–∞–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"

            # Dedup (safety-net, unlikely with OpenAI but harmless)
            final_text = _dedup_response(final_text)

            # Strip leaked LLM reasoning / JSON fragments from end of response
            final_text = _strip_reasoning_leak(final_text)

            # Sentence-level dedup (catches intra-paragraph question repeats)
            final_text = _dedup_sentences(final_text)

            # Strip orphaned dialogue-continuation fragments after last '?'
            final_text = _strip_trailing_fragment(final_text)

            # Strip leaked function names (e.g. "get_tour_details")
            final_text = _RE_FUNC_NAMES.sub('', final_text)
            final_text = re.sub(r'\s{2,}', ' ', final_text).strip()

            # Save to history
            self.full_history.append({
                "role": "assistant", "content": final_text
            })

            total_ms = int((time.perf_counter() - chat_start) * 1000)
            logger.info(
                "ü§ñ ASSISTANT << %d chars  %d iterations  %dms total  \"%s\"",
                len(final_text), iteration, total_ms,
                final_text[:200] + ("‚Ä¶" if len(final_text) > 200 else "")
            )
            return final_text

        logger.error("ü§ñ MAX ITERATIONS REACHED (%d)", max_iterations)
        return (
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –∑–∞–ø—Ä–æ—Å –æ–∫–∞–∑–∞–ª—Å—è —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–º. "
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã."
        )

    # ‚îÄ‚îÄ‚îÄ History Cleanup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _cleanup_history(self):
        """
        Remove invalid message sequences from full_history.
        Uses block grouping to keep tool_call/tool_result pairs atomic.
        """
        blocks = self._group_into_blocks(self.full_history)
        cleaned_blocks = []
        for block in blocks:
            msg = block[0]
            if msg.get("role") == "tool":
                logger.debug(
                    "üßπ CLEANUP: skipping orphaned tool message "
                    "tool_call_id=%s",
                    msg.get("tool_call_id", "?")
                )
                continue
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                tc_ids = {tc["id"] for tc in msg["tool_calls"]}
                found_ids = {
                    m.get("tool_call_id")
                    for m in block[1:]
                    if m.get("role") == "tool"
                }
                if tc_ids != found_ids:
                    logger.debug(
                        "üßπ CLEANUP: removing incomplete tool_call block "
                        "expected=%s found=%s",
                        tc_ids, found_ids
                    )
                    continue
            cleaned_blocks.append(block)

        cleaned = [msg for block in cleaned_blocks for msg in block]
        if len(cleaned) != len(self.full_history):
            logger.info(
                "üßπ CLEANUP: %d ‚Üí %d messages (removed %d invalid)",
                len(self.full_history), len(cleaned),
                len(self.full_history) - len(cleaned)
            )
        self.full_history = cleaned

    # ‚îÄ‚îÄ‚îÄ Streaming (fallback to non-streaming) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def chat_stream(
        self,
        user_message: str,
        on_token: Optional[StreamCallback] = None
    ) -> str:
        """
        Streaming not yet implemented for OpenAI.
        Falls back to regular chat().
        """
        logger.warning(
            "‚ö†Ô∏è chat_stream() fallback to chat() ‚Äî "
            "streaming –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –¥–ª—è OpenAI"
        )
        result = await self.chat(user_message)
        if on_token:
            on_token(result)
        return result

    # ‚îÄ‚îÄ‚îÄ Lifecycle ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def close_sync(self):
        """Close OpenAI client resources."""
        try:
            self.openai_client.close()
        except Exception:
            pass

    def reset(self):
        """Reset dialogue history and all caches."""
        old_len = len(self.full_history)
        self.full_history = []
        self.input_list = []
        self._pending_tour_cards = []
        self._pinned_context = None
        self._pinned_search_intent = None
        self._collected_slots = {}
        self._last_departure_city = "–ú–æ—Å–∫–≤–∞"
        self._last_requestid = None
        self._tourid_map = {}
        self._tour_details_cache = {}
        self._last_search_params = {}
        self._user_stated_budget = None
        self._empty_iterations = 0
        self.previous_response_id = None
        self._metrics = {
            "promised_search_detections": 0,
            "cascade_incomplete_detections": 0,
            "dateto_corrections": 0,
            "total_searches": 0,
            "total_messages": 0,
        }
        logger.info(
            "üîÑ HANDLER RESET  cleared %d messages from full_history",
            old_len
        )
